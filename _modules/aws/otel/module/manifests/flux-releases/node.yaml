apiVersion: helm.toolkit.fluxcd.io/v2beta2
kind: HelmRelease
metadata:
  name: otel-node-pod
  namespace: flux-releases-region
spec:
  interval: 10m
  timeout: 5m
  chart:
    spec:
      chart: raw
      version: "2.0.0"
      sourceRef:
        kind: HelmRepository
        name: bedag
        namespace: flux-repos
      interval: 5m
  releaseName: otel-node-pod
  targetNamespace: otel-system
  install:
    remediation:
      retries: 3
  upgrade:
    remediation:
      retries: 3
  test:
    enable: false
  dependsOn:
    - name: otel-operator
      namespace: flux-releases-region
  values:
    templates:
      - |
        apiVersion: opentelemetry.io/v1alpha1
        kind: OpenTelemetryCollector
        metadata:
          name: otel-node-pod
        spec:
          serviceAccount: otel-collector-cluster
          # image: otel/opentelemetry-collector-contrib:0.56.0
          mode: daemonset
          UpdateStrategy:
            type: RollingUpdate
            rollingUpdate:
              maxUnavailable: 50
          tolerations:
            - key: "ebs.csi.aws.com/agent-not-ready"
              operator: "Exists"
            - key: "efs.csi.aws.com/agent-not-ready"
              operator: "Exists"
          affinity:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                  - matchExpressions:
                    - key: beta.kubernetes.io/os
                      operator: In
                      values:
                      - linux
                    - key: eks.amazonaws.com/compute-type
                      operator: NotIn
                      values:
                      - fargate
          volumeMounts:
          - mountPath: /var/log/pods
            name: logs-pods
          - mountPath: /var/otel-col/
            name: otel-storage
          volumes:
          - name: logs-pods
            hostPath:
              path: /var/log/pods
              type: Directory
          - name: otel-storage
            hostPath:
              path: /var/otel-col/
              type: DirectoryOrCreate
          env:
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          config: |
            exporters:
              otlphttp:
                  endpoint: http://otel-cluster-partition-collector:4318
                  tls:
                    insecure: true
            receivers:
              filelog/containers:
                preserve_leading_whitespaces: true
                include:
                  - /var/log/pods/*/*/*.log
                start_at: beginning
                ## sets fingerprint_size to 17kb in order to match the longest possible docker line (which by default is 16kb)
                ## we want to include timestamp, which is at the end of the line
                fingerprint_size: 17408
                include_file_path: true
                include_file_name: false
                operators:
                  ## Detect the container runtime log format
                  ## Can be: docker-shim, CRI-O and containerd
                  - id: get-format
                    type: router
                    routes:
                      - output: parser-docker
                        expr: 'body matches "^\\{"'
                      - output: parser-crio
                        expr: 'body matches "^[^ Z]+ "'
                      - output: parser-containerd
                        expr: 'body matches "^[^ Z]+Z"'
                  ## Parse CRI-O format
                  - id: parser-crio
                    type: regex_parser
                    regex: "^(?P<time>[^ Z]+) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*)( |)(?P<log>.*)$"
                    output: merge-cri-lines
                    parse_to: body
                    timestamp:
                      parse_from: body.time
                      layout_type: gotime
                      layout: "2006-01-02T15:04:05.000000000-07:00"
                  ## Parse CRI-Containerd format
                  - id: parser-containerd
                    type: regex_parser
                    regex: "^(?P<time>[^ ^Z]+Z) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*)( |)(?P<log>.*)$"
                    output: merge-cri-lines
                    parse_to: body
                    timestamp:
                      parse_from: body.time
                      layout: "%Y-%m-%dT%H:%M:%S.%LZ"
                  ## Parse docker-shim format
                  ## parser-docker interprets the input string as JSON and moves the `time` field from the JSON to Timestamp field in the OTLP log
                  ## record.
                  ## Input Body (string): '{"log":"2001-02-03 04:05:06 first line\n","stream":"stdout","time":"2021-11-25T09:59:13.23887954Z"}'
                  ## Output Body (JSON): { "log": "2001-02-03 04:05:06 first line\n", "stream": "stdout" }
                  ## Input Timestamp: _empty_
                  ## Output Timestamp: 2021-11-25 09:59:13.23887954 +0000 UTC
                  - id: parser-docker
                    type: json_parser
                    parse_to: body
                    output: merge-docker-lines
                    timestamp:
                      parse_from: body.time
                      layout: "%Y-%m-%dT%H:%M:%S.%LZ"

                  ## merge-docker-lines stitches back together log lines split by Docker logging driver.
                  ## Input Body (JSON): { "log": "2001-02-03 04:05:06 very long li", "stream": "stdout" }
                  ## Input Body (JSON): { "log": "ne that was split by the logging driver\n", "stream": "stdout" }
                  ## Output Body (JSON): { "log": "2001-02-03 04:05:06 very long line that was split by the logging driver\n","stream":"stdout"}
                  - id: merge-docker-lines
                    type: recombine
                    source_identifier: attributes["log.file.path"]
                    output: merge-multiline-logs
                    combine_field: body.log
                    combine_with: ""
                    is_last_entry: body.log matches "\n$"

                  ## merge-cri-lines stitches back together log lines split by CRI logging drivers.
                  ## Input Body (JSON): { "log": "2001-02-03 04:05:06 very long li", "logtag": "P" }
                  ## Input Body (JSON): { "log": "ne that was split by the logging driver", "logtag": "F" }
                  ## Output Body (JSON): { "log": "2001-02-03 04:05:06 very long line that was split by the logging driver\n", "stream": "stdout" }
                  - id: merge-cri-lines
                    type: recombine
                    source_identifier: attributes["log.file.path"]
                    output: merge-multiline-logs
                    combine_field: body.log
                    combine_with: ""
                    is_last_entry: body.logtag == "F"
                    overwrite_with: newest

                  ## merge-multiline-logs merges incoming log records into multiline logs.
                  ## Input Body (JSON): { "log": "2001-02-03 04:05:06 first line\n", "stream": "stdout" }
                  ## Input Body (JSON): { "log": "  second line\n", "stream": "stdout" }
                  ## Input Body (JSON): { "log": "  third line\n", "stream": "stdout" }
                  ## Output Body (JSON): { "log": "2001-02-03 04:05:06 first line\n  second line\n  third line\n", "stream": "stdout" }
                  - id: merge-multiline-logs
                    type: recombine
                    output: extract-metadata-from-filepath
                    source_identifier: attributes["log.file.path"]
                    combine_field: body.log
                    combine_with: ""
                    is_first_entry: body.log matches "^\\[?\\d{4}-\\d{1,2}-\\d{1,2}.\\d{2}:\\d{2}:\\d{2}.*"

                  ## extract-metadata-from-filepath extracts data from the `log.file.path` Attribute into the Attributes
                  ## Input Attributes:
                  ## - log.file.path: '/var/log/pods/default_logger-multiline-4nvg4_aed49747-b541-4a07-8663-f7e1febc47d5/loggercontainer/0.log'
                  ## Output Attributes:
                  ## - log.file.path: '/var/log/pods/default_logger-multiline-4nvg4_aed49747-b541-4a07-8663-f7e1febc47d5/loggercontainer/0.log'
                  ## - container_name: "loggercontainer",
                  ## - namespace: "default",
                  ## - pod_name: "logger-multiline-4nvg4",
                  ## - run_id: "0",
                  ## - uid: "aed49747-b541-4a07-8663-f7e1febc47d5"
                  ## }
                  - id: extract-metadata-from-filepath
                    type: regex_parser
                    regex: '^.*\/(?P<namespace>[^_]+)_(?P<pod_name>[^_]+)_(?P<uid>[a-f0-9\-]+)\/(?P<container_name>[^\._]+)\/(?P<run_id>\d+)\.log$'
                    parse_from: attributes["log.file.path"]

                  ## The following actions are being performed:
                  ## - renaming attributes
                  ## - moving stream from body to attribtues
                  ## - using body.log as body
                  ## - create fluent.tag attribute in order to route in metadata pods
                  ## Input Body (JSON): {
                  ##   "log": "2001-02-03 04:05:06 loggerlog 1 first line\n",
                  ##   "stream": "stdout",
                  ## }
                  ## Output Body (String): "2001-02-03 04:05:06 loggerlog 1 first line\n"
                  ## Input Attributes:
                  ## - log.file.path: '/var/log/pods/default_logger-multiline-4nvg4_aed49747-b541-4a07-8663-f7e1febc47d5/loggercontainer/0.log'
                  ## - container_name: "loggercontainer",
                  ## - namespace: "default",
                  ## - pod_name: "logger-multiline-4nvg4",
                  ## - run_id: "0",
                  ## - uid: "aed49747-b541-4a07-8663-f7e1febc47d5"
                  ## Output Attributes:
                  ## - k8s.container.name: "loggercontainer"
                  ## - k8s.namespace.name: "default"
                  ## - k8s.pod.name: "logger-multiline-4nvg4"
                  ## - k8s.pod.uid: "aed49747-b541-4a07-8663-f7e1febc47d5"
                  ## - run_id: "0"
                  ## - stream: "stdout"
                  ## - fluent.tag: "containers.loggercontainer"
                  - id: move-attributes
                    type: move
                    from: body.stream
                    to: attributes["stream"]
                  - type: move
                    from: attributes.container_name
                    to: attributes["k8s.container.name"]
                  - type: move
                    from: attributes.namespace
                    to: attributes["k8s.namespace.name"]
                  - type: move
                    from: attributes.pod_name
                    to: attributes["k8s.pod.name"]
                  - type: move
                    from: attributes.run_id
                    to: attributes["run_id"]
                  - type: move
                    from: attributes.uid
                    to: attributes["k8s.pod.uid"]
                  - type: add
                    field: attributes["fluent.tag"]
                    value: EXPR("containers." + attributes["k8s.container.name"])
                  ## Use remove operator when available in opentelemetry collector:
                  ## https://github.com/open-telemetry/opentelemetry-collector-contrib/pull/9524
                  - type: move
                    from: attributes["log.file.path"]
                    to: body["log.file.path"]
                  - type: move
                    from: body.log
                    to: body

            processors:
              memory_limiter:
                check_interval: 1s
                limit_percentage: 75
                spike_limit_percentage: 15
              batch:
                send_batch_size: 1000
                timeout: 5s
              resourcedetection:
                detectors:
                - env
                - eks
                - ec2
                - system
                system:
                  hostname_sources: ["os"]
                timeout: 2s
                override: false
              k8sattributes:
            extensions:
              health_check:
                endpoint: 0.0.0.0:13133
              pprof:
                endpoint: :1888
              zpages:
                endpoint: :55679
              db_storage:
                driver: "sqlite3"
                datasource: "/var/otel-col/podlog.db?_busy_timeout=10000&_journal=WAL&_sync=NORMAL"
            service:
              extensions: [pprof, zpages, health_check]
              pipelines:
                logs:
                  receivers:
                  - filelog/containers
                  processors:
                  - resourcedetection
                  - k8sattributes
                  - batch
                  exporters: [otlphttp]
